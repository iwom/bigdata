{"paragraphs":[{"text":"%md\n# UWAGA\nProszę dodać Morpheusa do kernela; poniższa komórka nie działa","user":"anonymous","dateUpdated":"2020-05-15T06:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>UWAGA</h1>\n<p>Proszę dodać Morpheusa do kernela; poniższa komórka nie działa</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604280_1909216373","id":"paragraph_1589353236639_1295546417","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:23:33+0000","dateFinished":"2020-05-15T06:23:35+0000","status":"FINISHED","focus":true,"$$hashKey":"object:26321"},{"text":"%spark.dep\nz.load(\"org.opencypher:morpheus-spark-cypher:0.4.2\")","user":"anonymous","dateUpdated":"2020-05-15T06:23:42+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Interpreter spark.dep not found"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604281_1422003489","id":"paragraph_1589350795013_-995766314","dateCreated":"2020-05-15T06:20:04+0000","status":"ERROR","$$hashKey":"object:26322"},{"text":"import org.opencypher.okapi.api.util.ZeppelinSupport._","user":"anonymous","dateUpdated":"2020-05-15T06:26:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.opencypher.okapi.api.util.ZeppelinSupport._\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604281_1140820338","id":"paragraph_1589350844281_-124020422","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:26:07+0000","dateFinished":"2020-05-15T06:26:27+0000","status":"FINISHED","$$hashKey":"object:26323"},{"text":"%sh\nwget http://www.cs.put.poznan.pl/kjankiewicz/bigdata/cineasts_12k_movies_50k_actors_2.1.6.zip\nunzip cine*.zip","user":"anonymous","dateUpdated":"2020-05-15T06:26:31+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"--2020-05-15 06:26:32--  http://www.cs.put.poznan.pl/kjankiewicz/bigdata/cineasts_12k_movies_50k_actors_2.1.6.zip\nResolving www.cs.put.poznan.pl (www.cs.put.poznan.pl)... 150.254.30.30\nConnecting to www.cs.put.poznan.pl (www.cs.put.poznan.pl)|150.254.30.30|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3616506 (3.4M) [application/zip]\nSaving to: ‘cineasts_12k_movies_50k_actors_2.1.6.zip’\n\n     0K .......... .......... .......... .......... ..........  1%  415K 8s\n    50K .......... .......... .......... .......... ..........  2% 1.21M 6s\n   100K .......... .......... .......... .......... ..........  4% 2.39M 4s\n   150K .......... .......... .......... .......... ..........  5% 2.22M 3s\n   200K .......... .......... .......... .......... ..........  7% 2.43M 3s\n   250K .......... .......... .......... .......... ..........  8% 2.37M 3s\n   300K .......... .......... .......... .......... ..........  9% 96.1M 2s\n   350K .......... .......... .......... .......... .......... 11% 2.47M 2s\n   400K .......... .......... .......... .......... .......... 12% 80.6M 2s\n   450K .......... .......... .......... .......... .......... 14% 2.53M 2s\n   500K .......... .......... .......... .......... .......... 15% 70.3M 2s\n   550K .......... .......... .......... .......... .......... 16% 73.6M 1s\n   600K .......... .......... .......... .......... .......... 18% 2.56M 1s\n   650K .......... .......... .......... .......... .......... 19% 83.7M 1s\n   700K .......... .......... .......... .......... .......... 21% 82.7M 1s\n   750K .......... .......... .......... .......... .......... 22% 2.53M 1s\n   800K .......... .......... .......... .......... .......... 24%  123M 1s\n   850K .......... .......... .......... .......... .......... 25% 34.0M 1s\n   900K .......... .......... .......... .......... .......... 26% 2.71M 1s\n   950K .......... .......... .......... .......... .......... 28% 60.9M 1s\n  1000K .......... .......... .......... .......... .......... 29% 86.4M 1s\n  1050K .......... .......... .......... .......... .......... 31% 55.6M 1s\n  1100K .......... .......... .......... .......... .......... 32% 34.5M 1s\n  1150K .......... .......... .......... .......... .......... 33% 2.89M 1s\n  1200K .......... .......... .......... .......... .......... 35% 78.5M 1s\n  1250K .......... .......... .......... .......... .......... 36% 94.1M 1s\n  1300K .......... .......... .......... .......... .......... 38% 66.6M 1s\n  1350K .......... .......... .......... .......... .......... 39% 2.62M 1s\n  1400K .......... .......... .......... .......... .......... 41% 20.7M 1s\n  1450K .......... .......... .......... .......... .......... 42% 2.69M 1s\n  1500K .......... .......... .......... .......... .......... 43% 26.0M 1s\n  1550K .......... .......... .......... .......... .......... 45% 2.73M 1s\n  1600K .......... .......... .......... .......... .......... 46% 28.4M 0s\n  1650K .......... .......... .......... .......... .......... 48% 2.73M 0s\n  1700K .......... .......... .......... .......... .......... 49% 68.5M 0s\n  1750K .......... .......... .......... .......... .......... 50% 35.2M 0s\n  1800K .......... .......... .......... .......... .......... 52% 64.8M 0s\n  1850K .......... .......... .......... .......... .......... 53% 2.73M 0s\n  1900K .......... .......... .......... .......... .......... 55% 67.8M 0s\n  1950K .......... .......... .......... .......... .......... 56% 40.3M 0s\n  2000K .......... .......... .......... .......... .......... 58% 2.77M 0s\n  2050K .......... .......... .......... .......... .......... 59% 90.5M 0s\n  2100K .......... .......... .......... .......... .......... 60% 94.1M 0s\n  2150K .......... .......... .......... .......... .......... 62% 32.0M 0s\n  2200K .......... .......... .......... .......... .......... 63% 92.5M 0s\n  2250K .......... .......... .......... .......... .......... 65% 2.86M 0s\n  2300K .......... .......... .......... .......... .......... 66% 67.4M 0s\n  2350K .......... .......... .......... .......... .......... 67% 50.4M 0s\n  2400K .......... .......... .......... .......... .......... 69% 79.8M 0s\n  2450K .......... .......... .......... .......... .......... 70% 46.2M 0s\n  2500K .......... .......... .......... .......... .......... 72% 93.5M 0s\n  2550K .......... .......... .......... .......... .......... 73% 2.95M 0s\n  2600K .......... .......... .......... .......... .......... 75% 66.5M 0s\n  2650K .......... .......... .......... .......... .......... 76% 2.58M 0s\n  2700K .......... .......... .......... .......... .......... 77%  104M 0s\n  2750K .......... .......... .......... .......... .......... 79% 71.1M 0s\n  2800K .......... .......... .......... .......... .......... 80% 80.7M 0s\n  2850K .......... .......... .......... .......... .......... 82% 2.66M 0s\n  2900K .......... .......... .......... .......... .......... 83% 69.4M 0s\n  2950K .......... .......... .......... .......... .......... 84% 89.5M 0s\n  3000K .......... .......... .......... .......... .......... 86% 88.9M 0s\n  3050K .......... .......... .......... .......... .......... 87% 2.78M 0s\n  3100K .......... .......... .......... .......... .......... 89% 42.7M 0s\n  3150K .......... .......... .......... .......... .......... 90%  104M 0s\n  3200K .......... .......... .......... .......... .......... 92% 88.2M 0s\n  3250K .......... .......... .......... .......... .......... 93% 94.6M 0s\n  3300K .......... .......... .......... .......... .......... 94% 2.63M 0s\n  3350K .......... .......... .......... .......... .......... 96%  103M 0s\n  3400K .......... .......... .......... .......... .......... 97%  101M 0s\n  3450K .......... .......... .......... .......... .......... 99%  112M 0s\n  3500K .......... .......... .......... .                    100% 83.8M=0.6s\n\n2020-05-15 06:26:33 (5.66 MB/s) - ‘cineasts_12k_movies_50k_actors_2.1.6.zip’ saved [3616506/3616506]\n\nArchive:  cineasts_12k_movies_50k_actors_2.1.6.zip\n  inflating: acts_in.json            \n  inflating: directed.json           \n  inflating: movies.json             \n  inflating: persons.json            \n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604281_1909368122","id":"paragraph_1588745459610_2126928859","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:26:31+0000","dateFinished":"2020-05-15T06:26:33+0000","status":"FINISHED","$$hashKey":"object:26324"},{"text":"%sh\nhadoop fs -mkdir -p movies\nhadoop fs -copyFromLocal *.json movies\nhadoop fs -ls movies","user":"anonymous","dateUpdated":"2020-05-15T06:26:38+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 4 items\n-rw-r--r--   2 zeppelin hadoop    5452196 2020-05-15 06:26 movies/acts_in.json\n-rw-r--r--   2 zeppelin hadoop     412161 2020-05-15 06:26 movies/directed.json\n-rw-r--r--   2 zeppelin hadoop    5433721 2020-05-15 06:26 movies/movies.json\n-rw-r--r--   2 zeppelin hadoop    6502673 2020-05-15 06:26 movies/persons.json\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604282_1030236885","id":"paragraph_1588746228363_1238204421","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:26:38+0000","dateFinished":"2020-05-15T06:26:45+0000","status":"FINISHED","$$hashKey":"object:26325"},{"text":"%md\n## Load movies dataframe\nreduntant\n","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Load movies dataframe</h2>\n<p>reduntant</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604283_-453123890","id":"paragraph_1588761481280_-943896470","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26326"},{"text":"import spark.implicits._\n\nval moviesDF = spark.read.json(\"movies/movies.json\").cache()\nval moviesDFshort = moviesDF.where($\"_corrupt_record\".isNull).select(\"id\", \"title\")\nmoviesDFshort.show(5)","user":"anonymous","dateUpdated":"2020-05-15T06:27:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------------------+\n| id|               title|\n+---+--------------------+\n|345|              Avatar|\n|375|   Full Metal Jacket|\n|399|E.T.: The Extra-T...|\n|414|    Independence Day|\n|431|          The Matrix|\n+---+--------------------+\nonly showing top 5 rows\n\nimport spark.implicits._\n\u001b[1m\u001b[34mmoviesDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [_corrupt_record: string, description: string ... 9 more fields]\n\u001b[1m\u001b[34mmoviesDFshort\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: bigint, title: string]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604283_911088806","id":"paragraph_1588746393873_-1263682575","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:26:46+0000","dateFinished":"2020-05-15T06:26:56+0000","status":"FINISHED","$$hashKey":"object:26327"},{"text":"%md\n## Load persons dataframe","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Load persons dataframe</h2>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604283_-571234128","id":"paragraph_1588761511052_-339738668","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26328"},{"text":"val personsDF = spark.read.json(\"movies/persons.json\").cache()\nval personsDFshort = personsDF.where($\"_corrupt_record\".isNull).select(\"id\", \"name\")\npersonsDFshort.show(5)","user":"anonymous","dateUpdated":"2020-05-15T06:27:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------------+\n| id|            name|\n+---+----------------+\n|344|         Olliver|\n|346|   James Cameron|\n|347| Sam Worthington|\n|349|Sigourney Weaver|\n|350|    Stephen Lang|\n+---+----------------+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mpersonsDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [_corrupt_record: string, biography: string ... 5 more fields]\n\u001b[1m\u001b[34mpersonsDFshort\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: bigint, name: string]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604283_430091849","id":"paragraph_1588747013853_1163238676","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:26:52+0000","dateFinished":"2020-05-15T06:26:58+0000","status":"FINISHED","$$hashKey":"object:26329"},{"text":"%md\n## Load and merge relations\n`directed` and `acts_in` dataframes","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Load and merge relations</h2>\n<p><code>directed</code> and <code>acts_in</code> dataframes</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604284_-290319654","id":"paragraph_1588761546896_-710168630","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26330"},{"text":"val directedDF = spark.read.json(\"movies/directed.json\")//.where($\"_corrupt_record\".isNull)\ndirectedDF.show(5)","user":"anonymous","dateUpdated":"2020-05-15T06:27:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-----+\n|from_id|to_id|\n+-------+-----+\n|    346|16896|\n|    346| 6737|\n|    346| 6002|\n|    346| 3391|\n|    346| 2686|\n+-------+-----+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mdirectedDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [from_id: bigint, to_id: bigint]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604284_530113392","id":"paragraph_1588747128074_-865639948","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:27:09+0000","dateFinished":"2020-05-15T06:27:09+0000","status":"FINISHED","$$hashKey":"object:26331"},{"text":"val actsDF = spark.read.json(\"movies/acts_in.json\").cache()\n\nactsDF.where($\"_corrupt_record\".isNull).show(5)","user":"anonymous","dateUpdated":"2020-05-15T06:27:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+-------+---------+-----+\n|_corrupt_record|from_id|     name|to_id|\n+---------------+-------+---------+-----+\n|           null|    347|  Perseus|69157|\n|           null|    347|      Joe|64732|\n|           null|    347|PFC Lucas|57426|\n|           null|    347|  MacBeth|56600|\n|           null|    347|     Neil|54333|\n+---------------+-------+---------+-----+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mactsDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [_corrupt_record: string, from_id: bigint ... 2 more fields]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604284_313251249","id":"paragraph_1588747277593_-1336563272","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:27:14+0000","dateFinished":"2020-05-15T06:27:17+0000","status":"FINISHED","$$hashKey":"object:26332"},{"text":"val relations = directedDF.union(actsDF.where($\"_corrupt_record\".isNull).select(\"from_id\", \"to_id\")).distinct()\nrelations.count","user":"anonymous","dateUpdated":"2020-05-15T06:27:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mrelations\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [from_id: bigint, to_id: bigint]\n\u001b[1m\u001b[34mres5\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 103126\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604285_157503309","id":"paragraph_1588747497078_1844081447","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:27:22+0000","dateFinished":"2020-05-15T06:27:26+0000","status":"FINISHED","$$hashKey":"object:26333"},{"text":"%md\n## Process relations into list of pairs","user":"anonymous","dateUpdated":"2020-05-15T06:28:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Process relations into list of pairs</h2>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604285_-760460134","id":"paragraph_1588761612341_-261140251","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26334"},{"text":"val groupListDF = relations.groupBy(\"to_id\").agg(collect_set(\"from_id\").as(\"related\")).select(\"related\")","user":"anonymous","dateUpdated":"2020-05-15T06:27:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mgroupListDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [related: array<bigint>]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604285_1826540605","id":"paragraph_1588758884773_237665646","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:27:49+0000","dateFinished":"2020-05-15T06:27:49+0000","status":"FINISHED","$$hashKey":"object:26335"},{"text":"import org.apache.spark.sql.functions.udf\n\nval loopUDF = udf { x: Seq[String] => for (a <- x; b <-x if a < b) yield (a,b) }","user":"anonymous","dateUpdated":"2020-05-15T06:27:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.udf\n\u001b[1m\u001b[34mloopUDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = UserDefinedFunction($Lambda$3735/282198542@7d62f51e,ArrayType(StructType(StructField(_1,StringType,true), StructField(_2,StringType,true)),true),Some(List(ArrayType(StringType,true))))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604289_701122213","id":"paragraph_1588759633599_-1490064815","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:27:32+0000","dateFinished":"2020-05-15T06:27:33+0000","status":"FINISHED","$$hashKey":"object:26336"},{"text":"val pairsDF = groupListDF.withColumn(\"pairs\", loopUDF($\"related\")).select(explode($\"pairs\").as(\"pairs\")).select($\"pairs._1\".as(\"from\"), $\"pairs._2\".as(\"to\")).withColumn(\"id\", monotonically_increasing_id())","user":"anonymous","dateUpdated":"2020-05-15T06:27:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mpairsDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [from: string, to: string ... 1 more field]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604289_-835955082","id":"paragraph_1588760912035_-924270889","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:27:52+0000","dateFinished":"2020-05-15T06:27:52+0000","status":"FINISHED","$$hashKey":"object:26337"},{"text":"%md\n## Load to graph","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Load to graph</h2>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604289_-269701026","id":"paragraph_1588761706922_-1231649433","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26338"},{"text":"import org.opencypher.morpheus.api.MorpheusSession\nimport org.opencypher.morpheus.api.io.MorpheusElementTable\nimport org.opencypher.okapi.api.io.conversion.{NodeMappingBuilder, RelationshipMappingBuilder}\nimport org.opencypher.okapi.api.util.ZeppelinSupport._","user":"anonymous","dateUpdated":"2020-05-15T06:38:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.opencypher.morpheus.api.MorpheusSession\nimport org.opencypher.morpheus.api.io.MorpheusElementTable\nimport org.opencypher.okapi.api.io.conversion.{NodeMappingBuilder, RelationshipMappingBuilder}\nimport org.opencypher.okapi.api.util.ZeppelinSupport._\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604289_-664803487","id":"paragraph_1588761924923_-379437796","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:28:07+0000","dateFinished":"2020-05-15T06:28:07+0000","status":"FINISHED","$$hashKey":"object:26339"},{"text":"val peopleMapping = NodeMappingBuilder\n    .withSourceIdKey(\"id\")\n    .withImpliedLabel(\"Person\")\n    .withPropertyKey(propertyKey = \"name\", sourcePropertyKey = \"name\")\n    .build\n\nval relationsMapping = RelationshipMappingBuilder\n    .withSourceIdKey(\"id\")\n    .withSourceStartNodeKey(\"from\")\n    .withSourceEndNodeKey(\"to\")\n    .withRelType(\"RELATED\")\n    .build","user":"anonymous","dateUpdated":"2020-05-15T06:38:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mpeopleMapping\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.api.io.conversion.ElementMapping\u001b[0m = ElementMapping(NodePattern(NODE(:Person)),Map(PatternElement(node,NODE(:Person)) -> Map(name -> name)),Map(PatternElement(node,NODE(:Person)) -> Map(SourceIdKey -> id)))\n\u001b[1m\u001b[34mrelationsMapping\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.api.io.conversion.ElementMapping\u001b[0m = ElementMapping(RelationshipPattern(RELATIONSHIP(:RELATED)),Map(PatternElement(rel,RELATIONSHIP(:RELATED)) -> Map()),Map(PatternElement(rel,RELATIONSHIP(:RELATED)) -> Map(SourceIdKey -> id, SourceStartNodeKey -> from, SourceEndNodeKey -> to)))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604290_1863159688","id":"paragraph_1588748055557_-2080319819","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:28:10+0000","dateFinished":"2020-05-15T06:28:10+0000","status":"FINISHED","$$hashKey":"object:26340"},{"text":"implicit val morpheus: MorpheusSession = MorpheusSession.create(spark)","user":"anonymous","dateUpdated":"2020-05-15T06:38:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mmorpheus\u001b[0m: \u001b[1m\u001b[32morg.opencypher.morpheus.api.MorpheusSession\u001b[0m = MorpheusSession\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604290_802236023","id":"paragraph_1588762049085_-1307571910","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:28:13+0000","dateFinished":"2020-05-15T06:28:13+0000","status":"FINISHED","$$hashKey":"object:26341"},{"text":"val peopleTable = MorpheusElementTable.create(peopleMapping, personsDFshort)\nval relationsTable = MorpheusElementTable.create(relationsMapping, pairsDF)\n\nval graph = morpheus.readFrom(peopleTable, relationsTable)","user":"anonymous","dateUpdated":"2020-05-15T06:38:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mpeopleTable\u001b[0m: \u001b[1m\u001b[32morg.opencypher.morpheus.api.io.MorpheusElementTable\u001b[0m = MorpheusElementTable(ElementMapping(NodePattern(NODE(:Person)),Map(PatternElement(node,NODE(:Person)) -> Map(name -> name)),Map(PatternElement(node,NODE(:Person)) -> Map(SourceIdKey -> id))),org.opencypher.morpheus.impl.table.SparkTable$DataFrameTable@163e3202)\n\u001b[1m\u001b[34mrelationsTable\u001b[0m: \u001b[1m\u001b[32morg.opencypher.morpheus.api.io.MorpheusElementTable\u001b[0m = MorpheusElementTable(ElementMapping(RelationshipPattern(RELATIONSHIP(:RELATED)),Map(PatternElement(rel,RELATIONSHIP(:RELATED)) -> Map()),Map(PatternElement(rel,RELATIONSHIP(:RELATED)) -> Map(SourceIdKey -> id, SourceStartNodeKey -> from, SourceEndNodeKey -> to))),org.opencypher.morpheus.impl.table.SparkTable$DataFrameTable@6205b82b)\n\u001b[1m\u001b[34mgr...\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604290_1880915867","id":"paragraph_1588761940056_-777477666","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:28:15+0000","dateFinished":"2020-05-15T06:28:16+0000","status":"FINISHED","$$hashKey":"object:26342"},{"text":"%md\n# Braki w kompatybilności między Zeppelinem a Morpheusem\n[Dokumentacja](https://github.com/opencypher/morpheus/wiki/Use-CAPS-in-a-Zeppelin-notebook)  \nPróby wyświetlenie wyniku prostej kwerendy","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Braki w kompatybilności między Zeppelinem a Morpheusem</h1>\n<p><a href=\"https://github.com/opencypher/morpheus/wiki/Use-CAPS-in-a-Zeppelin-notebook\">Dokumentacja</a><br/>Próby wyświetlenie wyniku prostej kwerendy</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604290_-1507073602","id":"paragraph_1589350942916_1798294109","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26343"},{"text":"val schema1 = graph.cypher(\n    \"MATCH (n:Person) RETURN n.name LIMIT 100\"\n)","user":"anonymous","dateUpdated":"2020-05-15T06:37:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":8,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mschema1\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(n.name :: STRING?), solved=SolvedQueryModel(Set(n :: NODE(:Person) @ session.tmp1, n.name :: STRING?),Set(n:Person :: BOOLEAN)))),Some(Select(expressions=List(n.name :: STRING?), columnRenames=Map())))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604290_1727907786","id":"paragraph_1588762033611_1390485807","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:28:20+0000","dateFinished":"2020-05-15T06:28:22+0000","status":"FINISHED","$$hashKey":"object:26344"},{"text":"schema1.records.table.df.printSchema()","user":"anonymous","dateUpdated":"2020-05-15T06:37:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":4,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- n_name: string (nullable = true)\n\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604291_-429849615","id":"paragraph_1589351153080_-1291738172","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:30:23+0000","dateFinished":"2020-05-15T06:30:24+0000","status":"FINISHED","$$hashKey":"object:26345"},{"text":"schema1.records.printTable()","user":"anonymous","dateUpdated":"2020-05-15T06:30:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 220, cluster1-kafka-w-0.europe-west3-c.c.big-data-put.internal, executor 1): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.rdd.RDD.collectPartition$1(RDD.scala:1005)\n  at org.apache.spark.rdd.RDD.$anonfun$toLocalIterator$3(RDD.scala:1007)\n  at org.apache.spark.rdd.RDD.$anonfun$toLocalIterator$3$adapted(RDD.scala:1007)\n  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at scala.collection.Iterator.foreach(Iterator.scala:941)\n  at scala.collection.Iterator.foreach$(Iterator.scala:941)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.addString(TraversableOnce.scala:360)\n  at scala.collection.TraversableOnce.addString$(TraversableOnce.scala:356)\n  at scala.collection.AbstractIterator.addString(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.mkString(TraversableOnce.scala:326)\n  at scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:325)\n  at scala.collection.AbstractIterator.mkString(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.mkString(TraversableOnce.scala:328)\n  at scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:328)\n  at scala.collection.AbstractIterator.mkString(Iterator.scala:1429)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.toZeppelinTable(ZeppelinSupport.scala:129)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.printTable(ZeppelinSupport.scala:82)\n  ... 50 elided\nCaused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604291_-1442529757","id":"paragraph_1589350640700_1931652983","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:28:25+0000","dateFinished":"2020-05-15T06:28:27+0000","status":"ERROR","$$hashKey":"object:26346"},{"text":"schema1.records.table.df.show(7)","user":"anonymous","dateUpdated":"2020-05-15T06:30:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------+\n|            n_name|\n+------------------+\n|           Olliver|\n|     James Cameron|\n|   Sam Worthington|\n|  Sigourney Weaver|\n|      Stephen Lang|\n|Michelle Rodriguez|\n|        Joel Moore|\n+------------------+\nonly showing top 7 rows\n\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604292_-264140916","id":"paragraph_1589351519888_480688612","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26347"},{"text":"%md\nW powyzszym schemacie widać, ze metoda dostarczona w dokumentacji nie działa, natomiast bezposrednie odwołanie zdaje się działać.  \nPoniżej kolejny przykład.","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>W powyzszym schemacie widać, ze metoda dostarczona w dokumentacji nie działa, natomiast bezposrednie odwołanie zdaje się działać.<br/>Poniżej kolejny przykład.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604292_1426168451","id":"paragraph_1589351633132_-1918573786","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26348"},{"text":"val schema2 = graph.cypher(\n    \"MATCH (n:Person) RETURN n LIMIT 100\"\n)","user":"anonymous","dateUpdated":"2020-05-15T06:37:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":8,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mschema2\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(n :: NODE(:Person) @ session.tmp2), solved=SolvedQueryModel(Set(n :: NODE(:Person) @ session.tmp2),Set(n:Person :: BOOLEAN)))),Some(Select(expressions=List(n.name :: STRING?, n:Person :: BOOLEAN, n :: NODE(:Person) @ session.tmp2), columnRenames=Map())))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604292_-1967866589","id":"paragraph_1589351616034_-1579109634","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26349"},{"text":"schema2.records.table.df.printSchema()","user":"anonymous","dateUpdated":"2020-05-15T06:37:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":4,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- n_name: string (nullable = true)\n |-- n:Person: boolean (nullable = false)\n |-- n: binary (nullable = true)\n\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604295_557275145","id":"paragraph_1589351696765_-2023554933","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26350"},{"text":"schema2.records.printTable()","user":"anonymous","dateUpdated":"2020-05-15T06:32:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 20.0 failed 4 times, most recent failure: Lost task 1.3 in stage 20.0 (TID 438, klaster-w-0.europe-west3-c.c.lab0-wprowadzenie.internal, executor 2): java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n\tat org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat org.spark_project.guava.cache.LocalCache$LoadingValueReference.waitForValue(LocalCache.java:3620)\n\tat org.spark_project.guava.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2362)\n\tat org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2251)\n\tat org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)\n\tat org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n\tat org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1238)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:631)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:630)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:875)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:875)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1302)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1383)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1380)\n\tat org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 23 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.rdd.RDD.collectPartition$1(RDD.scala:1005)\n  at org.apache.spark.rdd.RDD.$anonfun$toLocalIterator$3(RDD.scala:1007)\n  at org.apache.spark.rdd.RDD.$anonfun$toLocalIterator$3$adapted(RDD.scala:1007)\n  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at scala.collection.Iterator.foreach(Iterator.scala:941)\n  at scala.collection.Iterator.foreach$(Iterator.scala:941)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.addString(TraversableOnce.scala:360)\n  at scala.collection.TraversableOnce.addString$(TraversableOnce.scala:356)\n  at scala.collection.AbstractIterator.addString(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.mkString(TraversableOnce.scala:326)\n  at scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:325)\n  at scala.collection.AbstractIterator.mkString(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.mkString(TraversableOnce.scala:328)\n  at scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:328)\n  at scala.collection.AbstractIterator.mkString(Iterator.scala:1429)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.toZeppelinTable(ZeppelinSupport.scala:129)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.printTable(ZeppelinSupport.scala:82)\n  ... 54 elided\nCaused by: java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n  at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n  at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n  at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n  at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n  at org.spark_project.guava.cache.LocalCache$LoadingValueReference.waitForValue(LocalCache.java:3620)\n  at org.spark_project.guava.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2362)\n  at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2251)\n  at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)\n  at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n  at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1238)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:631)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:630)\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:875)\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:875)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\nCaused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1302)\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1383)\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1380)\n  at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n  at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n  at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n  at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n  ... 23 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604296_119715920","id":"paragraph_1589351747230_-757015314","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26351"},{"text":"schema2.records.table.df.show(7)","user":"anonymous","dateUpdated":"2020-05-15T06:32:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 454, klaster-w-0.europe-west3-c.c.lab0-wprowadzenie.internal, executor 2): java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n\tat org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat org.spark_project.guava.cache.LocalCache$LoadingValueReference.waitForValue(LocalCache.java:3620)\n\tat org.spark_project.guava.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2362)\n\tat org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2251)\n\tat org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)\n\tat org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n\tat org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1238)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:631)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:630)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:875)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:875)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1302)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1383)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1380)\n\tat org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 23 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n  ... 54 elided\nCaused by: java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n  at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n  at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n  at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n  at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n  at org.spark_project.guava.cache.LocalCache$LoadingValueReference.waitForValue(LocalCache.java:3620)\n  at org.spark_project.guava.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2362)\n  at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2251)\n  at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)\n  at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n  at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1238)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:631)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:630)\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:875)\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:875)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\nCaused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 51, Column 31: Unknown variable or type \"org.opencypher.morpheus.impl.expressions.EncodeLong\"\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1302)\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1383)\n  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1380)\n  at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n  at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n  at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n  at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n  ... 23 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604296_1993600548","id":"paragraph_1589351763950_-1905758618","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26352"},{"text":"%md\nPrzy innym typie nie można już przekazać wyniku na wyjście","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Przy innym typie nie można już przekazać wyniku na wyjście</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604297_-240688474","id":"paragraph_1589351804240_617145348","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26353"},{"text":"%md\n## 1. Spójność w grafie\nZnaleziono \"lokalne\" rozwiązanie problemu. Mianowicie poszukujemy węzłów które posiadają 0 krawędzi - takich, które nie są w relacji do żadnego innego węzła.\nJeżeli istnieje choć jeden taki węzeł, oznacza to, że graf jest niespójny.","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1. Spójność w grafie</h2>\n<p>Znaleziono &ldquo;lokalne&rdquo; rozwiązanie problemu. Mianowicie poszukujemy węzłów które posiadają 0 krawędzi - takich, które nie są w relacji do żadnego innego węzła.<br/>Jeżeli istnieje choć jeden taki węzeł, oznacza to, że graf jest niespójny.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604298_-439869390","id":"paragraph_1589351903251_-1199170190","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26354"},{"text":" val consistency = graph.cypher(\n    \"\"\"|MATCH (n:Person)\n        |WHERE NOT (n)-[:RELATED]->()\n        |RETURN\n        |CASE\n        |WHEN COUNT(DISTINCT n)>0 THEN 'Graf niespójny'\n        |ELSE 'Graf spójny' END AS result\n        |\"\"\".stripMargin\n  )","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mconsistency\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(result :: STRING), solved=SolvedQueryModel(Set(n :: NODE(:Person) @ session.tmp4,   AGGREGATION63 :: INTEGER,   FRESHID53 :: STRING, result :: STRING),Set(n:Person :: BOOLEAN, NOT(Exists(  UNNAMED27 :: BOOLEAN)) :: BOOLEAN)))),Some(Select(expressions=List(result :: STRING), columnRenames=Map())))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604298_2056128070","id":"paragraph_1589378401763_-1414316057","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26355"},{"text":"consistency.show","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:658)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:166)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:129)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:150)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:123)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.DeserializeToObjectExec.inputRDDs(objects.scala:76)\n  at org.apache.spark.sql.execution.MapElementsExec.inputRDDs(objects.scala:207)\n  at org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:110)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2788)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)\n  at org.opencypher.morpheus.impl.RecordBehaviour.collect(MorpheusRecords.scala:135)\n  at org.opencypher.morpheus.impl.RecordBehaviour.collect$(MorpheusRecords.scala:134)\n  at org.opencypher.morpheus.impl.MorpheusRecords.collect(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.impl.table.RecordsPrinter$.print(RecordsPrinter.scala:44)\n  at org.opencypher.okapi.relational.api.table.RelationalCypherRecords.show(RelationalCypherRecords.scala:71)\n  at org.opencypher.okapi.relational.api.table.RelationalCypherRecords.show$(RelationalCypherRecords.scala:70)\n  at org.opencypher.morpheus.impl.MorpheusRecords.show(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.relational.api.planning.RelationalCypherResult.show(RelationalCypherResult.scala:77)\n  ... 54 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 465, klaster-w-0.europe-west3-c.c.lab0-wprowadzenie.internal, executor 2): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604298_421245943","id":"paragraph_1589381962722_-1815837584","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26356"},{"text":"%md\nJak widać, w środowisku Zeppelin użycie metody show powoduje błąd. W środowisku lokalnym cały proces sprawdzenia spójności grafu przebiega bezproblemowo.\nWynikiem powyższych poleceń jest informacja, że graf jest niespójny.\nTakie rozwiązanie jest rozwiązaniem \"lokalnym\", ponieważ w obecnym zbiorze danych znajdują się węzły (osoby) bez krawędzi (brakiem relacji).\nNie oznacza to, że rozwiązanie zadziała w każdym przypadku.\nIstnieje możliwiość, że w grafie znajduję się np. 5 osób które są w relacji tylko względem siebie, a względem pozostałych są wyalienowane.","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Jak widać, w środowisku Zeppelin użycie metody show powoduje błąd. W środowisku lokalnym cały proces sprawdzenia spójności grafu przebiega bezproblemowo.<br/>Wynikiem powyższych poleceń jest informacja, że graf jest niespójny.<br/>Takie rozwiązanie jest rozwiązaniem &ldquo;lokalnym&rdquo;, ponieważ w obecnym zbiorze danych znajdują się węzły (osoby) bez krawędzi (brakiem relacji).<br/>Nie oznacza to, że rozwiązanie zadziała w każdym przypadku.<br/>Istnieje możliwiość, że w grafie znajduję się np. 5 osób które są w relacji tylko względem siebie, a względem pozostałych są wyalienowane.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604299_-1971334326","id":"paragraph_1589379212126_-796751032","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26357"},{"text":"%md\n## 2. Jeden z mniejszych podgrafow\nDla rozwiązania z punktu pierwszego, w takim przypadku należy wypisać węzły \"wyalienowane\". Najmniejszym takim połączeniem jest pojedyńczy węzeł.\nW związku z tym wystarczy wypisać nazwiska tychże osób","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>2. Jeden z mniejszych podgrafow</h2>\n<p>Dla rozwiązania z punktu pierwszego, w takim przypadku należy wypisać węzły &ldquo;wyalienowane&rdquo;. Najmniejszym takim połączeniem jest pojedyńczy węzeł.<br/>W związku z tym wystarczy wypisać nazwiska tychże osób</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604299_-1170768041","id":"paragraph_1589351934837_816419006","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26358"},{"text":"  val aliens = graph.cypher(\r\n      \"\"\"\r\n      |MATCH (n:Person)\r\n      |WHERE NOT (n)-[:RELATED]->()\r\n      |RETURN n.name as Name\r\n      |\"\"\".stripMargin\r\n  )","user":"anonymous","dateUpdated":"2020-05-15T06:22:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34maliens\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(Name :: STRING?), solved=SolvedQueryModel(Set(n :: NODE(:Person) @ session.tmp5, Name :: STRING?),Set(n:Person :: BOOLEAN, NOT(Exists(  UNNAMED30 :: BOOLEAN)) :: BOOLEAN)))),Some(Select(expressions=List(Name :: STRING?), columnRenames=Map())))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604300_1701713853","id":"paragraph_1589380279012_1291288345","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26359"},{"text":"aliens.show","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:658)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:166)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:129)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:150)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:123)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n  at org.apache.spark.sql.execution.DeserializeToObjectExec.inputRDDs(objects.scala:76)\n  at org.apache.spark.sql.execution.MapElementsExec.inputRDDs(objects.scala:207)\n  at org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:110)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2788)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)\n  at org.opencypher.morpheus.impl.RecordBehaviour.collect(MorpheusRecords.scala:135)\n  at org.opencypher.morpheus.impl.RecordBehaviour.collect$(MorpheusRecords.scala:134)\n  at org.opencypher.morpheus.impl.MorpheusRecords.collect(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.impl.table.RecordsPrinter$.print(RecordsPrinter.scala:44)\n  at org.opencypher.okapi.relational.api.table.RelationalCypherRecords.show(RelationalCypherRecords.scala:71)\n  at org.opencypher.okapi.relational.api.table.RelationalCypherRecords.show$(RelationalCypherRecords.scala:70)\n  at org.opencypher.morpheus.impl.MorpheusRecords.show(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.relational.api.planning.RelationalCypherResult.show(RelationalCypherResult.scala:77)\n  ... 54 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 485, klaster-w-0.europe-west3-c.c.lab0-wprowadzenie.internal, executor 2): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604300_-542993223","id":"paragraph_1589380334027_205925139","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26360"},{"text":"%md\nPodobnie jak wcześniej, metoda show w środowisku Zeppelin nie działa poprawnie.\nW środowisku lokalnym takie polecenie nie powoduje problemów.\nWęzłów, które nie są w relacji do żadnego innego węzła jest ponad 4 tysiące.","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Podobnie jak wcześniej, metoda show w środowisku Zeppelin nie działa poprawnie.<br/>W środowisku lokalnym takie polecenie nie powoduje problemów.<br/>Węzłów, które nie są w relacji do żadnego innego węzła jest ponad 4 tysiące.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604301_1289289316","id":"paragraph_1589380382833_1942873938","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26361"},{"text":"%md \n## 3. Rozkład stopni wierzchołków","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3. Rozkład stopni wierzchołków</h2>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604301_1644169490","id":"paragraph_1588762240527_-1352229928","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26362"},{"text":"   val degrees = graph.cypher(\n    \"\"\"|MATCH (u:Person)-[r:RELATED]-()\n       |WITH u, COUNT(r) AS degree\n       |RETURN degree,\n       |COUNT(*) AS quantity\n       |ORDER BY degree DESC\n       |\"\"\".stripMargin\n  )","user":"anonymous","dateUpdated":"2020-05-15T06:39:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mdegrees\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(degree :: INTEGER, quantity :: INTEGER), solved=SolvedQueryModel(Set(degree :: INTEGER,   UNNAMED30 :: NODE() @ session.tmp2,   FRESHID74 :: INTEGER, r :: RELATIONSHIP(:RELATED) @ session.tmp2,   FRESHID66 :: INTEGER, u :: NODE(:Person) @ session.tmp2,   degree@52 :: INTEGER, quantity :: INTEGER),Set(u:Person :: BOOLEAN, r:RELATED :: BOOLEAN)))),Some(Select(expressions=List(degree :: INTEGER, quantity :: INTEGER), columnRenames=Map())))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604305_-978389123","id":"paragraph_1589294498172_-629895264","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:38:58+0000","dateFinished":"2020-05-15T06:38:58+0000","status":"FINISHED","$$hashKey":"object:26363"},{"text":"%md\n Próba wyświetlenia danych z wykorzystaniem funkcjonalności Apache Spark do wizualizacji wyników zapytań Cypher\n https://github.com/opencypher/morpheus/wiki/Use-CAPS-in-a-Zeppelin-notebook?fbclid=IwAR1L7ZwnrJtxn0vr-t2u9VIJsU-bws-Ab0fLNaAIvGcaaM7DiDBZZTb5ghU","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Próba wyświetlenia danych z wykorzystaniem funkcjonalności Apache Spark do wizualizacji wyników zapytań Cypher<br/> <a href=\"https://github.com/opencypher/morpheus/wiki/Use-CAPS-in-a-Zeppelin-notebook?fbclid=IwAR1L7ZwnrJtxn0vr-t2u9VIJsU-bws-Ab0fLNaAIvGcaaM7DiDBZZTb5ghU\">https://github.com/opencypher/morpheus/wiki/Use-CAPS-in-a-Zeppelin-notebook?fbclid=IwAR1L7ZwnrJtxn0vr-t2u9VIJsU-bws-Ab0fLNaAIvGcaaM7DiDBZZTb5ghU</a></p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604308_-1907307256","id":"paragraph_1589358832915_-1363079409","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26364"},{"text":"degrees.records.printTable()","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:590)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:237)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n  at scala.collection.immutable.List.map(List.scala:298)\n  at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:590)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:129)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n  at org.apache.spark.sql.execution.DeserializeToObjectExec.inputRDDs(objects.scala:76)\n  at org.apache.spark.sql.execution.MapElementsExec.inputRDDs(objects.scala:207)\n  at org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:110)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeToIterator(SparkPlan.scala:318)\n  at org.apache.spark.sql.Dataset.$anonfun$toLocalIterator$1(Dataset.scala:2822)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.toLocalIterator(Dataset.scala:2817)\n  at org.opencypher.morpheus.impl.RecordBehaviour.toLocalIterator(MorpheusRecords.scala:127)\n  at org.opencypher.morpheus.impl.RecordBehaviour.toLocalIterator$(MorpheusRecords.scala:126)\n  at org.opencypher.morpheus.impl.MorpheusRecords.toLocalIterator(MorpheusRecords.scala:91)\n  at org.opencypher.morpheus.impl.RecordBehaviour.iterator(MorpheusRecords.scala:123)\n  at org.opencypher.morpheus.impl.RecordBehaviour.iterator$(MorpheusRecords.scala:122)\n  at org.opencypher.morpheus.impl.MorpheusRecords.iterator(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.toZeppelinTable(ZeppelinSupport.scala:127)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.printTable(ZeppelinSupport.scala:82)\n  ... 48 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 223, cypher-w-1.europe-west3-c.c.my-private-project-270620.internal, executor 2): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604308_138925748","id":"paragraph_1589358923154_-954074137","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26365"},{"text":"degrees.records.printGraph()","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:590)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:237)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n  at scala.collection.immutable.List.map(List.scala:298)\n  at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:590)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:129)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n  at org.apache.spark.sql.execution.DeserializeToObjectExec.inputRDDs(objects.scala:76)\n  at org.apache.spark.sql.execution.MapElementsExec.inputRDDs(objects.scala:207)\n  at org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:110)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2788)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)\n  at org.opencypher.morpheus.impl.RecordBehaviour.collect(MorpheusRecords.scala:135)\n  at org.opencypher.morpheus.impl.RecordBehaviour.collect$(MorpheusRecords.scala:134)\n  at org.opencypher.morpheus.impl.MorpheusRecords.collect(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.toZeppelinGraph(ZeppelinSupport.scala:148)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.printGraph(ZeppelinSupport.scala:110)\n  ... 48 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 4 times, most recent failure: Lost task 0.3 in stage 13.0 (TID 237, cypher-w-1.europe-west3-c.c.my-private-project-270620.internal, executor 2): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604309_1603720616","id":"paragraph_1589359956421_1560540942","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26366"},{"text":"%md \nPróba prezentacji rozkładu stopni, z wykorzystaniem narzędzi do wizualizacji dostępnych w Zeppelinie","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Próba prezentacji rozkładu stopni, z wykorzystaniem narzędzi do wizualizacji dostępnych w Zeppelinie</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604310_-1044578563","id":"paragraph_1589360015873_-647740125","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26367"},{"text":"<!--Przekształcenie grafu do DataFrame-->\n\nval degreeDistributionDF = degrees.records.table.df\ndegreeDistributionDF.printSchema()\n\ndegreeDistributionDF.createOrReplaceTempView(\"degreeD\")","user":"anonymous","dateUpdated":"2020-05-15T06:39:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- degree: long (nullable = false)\n |-- quantity: long (nullable = false)\n\n\u001b[1m\u001b[34mdegreeDistributionDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [degree: bigint, quantity: bigint]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604310_-1984123415","id":"paragraph_1589351268938_-1062191359","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:39:20+0000","dateFinished":"2020-05-15T06:39:21+0000","status":"FINISHED","$$hashKey":"object:26368"},{"text":"%sql show tables","user":"anonymous","dateUpdated":"2020-05-15T06:36:54+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"database":"string","tableName":"string","isTemporary":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"database\ttableName\tisTemporary\n\tdegreed\ttrue\n"},{"type":"TEXT","data":""}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604310_1820953843","id":"paragraph_1589294511062_-1732984465","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26369"},{"text":"%sql select * from degreed","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"database":"string","tableName":"string","isTemporary":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"scatterChart":{"yAxis":{"name":"tableName","index":1,"aggr":"sum"}}},"keys":[{"name":"database","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"tableName","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Job aborted due to stage failure: Task 0 in stage 25.0 failed 4 times, most recent failure: Lost task 0.3 in stage 25.0 (TID 272, klaster-w-0.europe-west3-c.c.lab0-wprowadzenie.internal, executor 2): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604311_1733874043","id":"paragraph_1589294563127_1941772497","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26370"},{"text":"%md\nPróba wyświetlenie dataframe'a wszystkimi sposobami, wszędzie pojawia się błąd dotyczący castowania typow\nPoniżej uproszczona kwerenda też zwraca podobny błąd\nTen błąd zdaje się występować na etapie grupowania\nW środowisku lokalnym (Spark 3) takowy błąd nie występuje","user":"anonymous","dateUpdated":"2020-05-15T06:20:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Próba wyświetlenie dataframe&rsquo;a wszystkimi sposobami, wszędzie pojawia się błąd dotyczący castowania typow<br/>Poniżej uproszczona kwerenda też zwraca podobny błąd<br/>Ten błąd zdaje się występować na etapie grupowania<br/>W środowisku lokalnym (Spark 3) takowy błąd nie występuje</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604311_500473207","id":"paragraph_1589353032516_1485913038","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26371"},{"text":"val degrees2 = graph.cypher(\n    \"\"\"|MATCH (u:Person)-[r:RELATED]-()\n       |WITH u, COUNT(r) AS degree\n       |RETURN u.name, degree\n       \"\"\".stripMargin)\n","user":"anonymous","dateUpdated":"2020-05-15T06:36:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":8,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mdegrees2\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(u.name :: STRING?, degree :: INTEGER), solved=SolvedQueryModel(Set(u.name :: STRING?, degree :: INTEGER,   UNNAMED30 :: NODE() @ session.tmp5, r :: RELATIONSHIP(:RELATED) @ session.tmp5, u :: NODE(:Person) @ session.tmp5),Set(u:Person :: BOOLEAN, r:RELATED :: BOOLEAN)))),Some(Select(expressions=List(u.name :: STRING?, degree :: INTEGER), columnRenames=Map())))\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604311_2115497579","id":"paragraph_1589295135313_-1633316873","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26372"},{"text":"degrees2.records.table.df.printSchema()","user":"anonymous","dateUpdated":"2020-05-15T06:39:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":4,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- u_name: string (nullable = true)\n |-- degree: long (nullable = false)\n\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604311_-295493552","id":"paragraph_1589352433958_-752546050","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26373"},{"text":"degrees2.records.printTable()","user":"anonymous","dateUpdated":"2020-05-15T06:33:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:590)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:237)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n  at scala.collection.immutable.List.map(List.scala:298)\n  at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:590)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.DeserializeToObjectExec.inputRDDs(objects.scala:76)\n  at org.apache.spark.sql.execution.MapElementsExec.inputRDDs(objects.scala:207)\n  at org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:110)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeToIterator(SparkPlan.scala:318)\n  at org.apache.spark.sql.Dataset.$anonfun$toLocalIterator$1(Dataset.scala:2822)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.toLocalIterator(Dataset.scala:2817)\n  at org.opencypher.morpheus.impl.RecordBehaviour.toLocalIterator(MorpheusRecords.scala:127)\n  at org.opencypher.morpheus.impl.RecordBehaviour.toLocalIterator$(MorpheusRecords.scala:126)\n  at org.opencypher.morpheus.impl.MorpheusRecords.toLocalIterator(MorpheusRecords.scala:91)\n  at org.opencypher.morpheus.impl.RecordBehaviour.iterator(MorpheusRecords.scala:123)\n  at org.opencypher.morpheus.impl.RecordBehaviour.iterator$(MorpheusRecords.scala:122)\n  at org.opencypher.morpheus.impl.MorpheusRecords.iterator(MorpheusRecords.scala:91)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.toZeppelinTable(ZeppelinSupport.scala:127)\n  at org.opencypher.okapi.api.util.ZeppelinSupport$ZeppelinRecords.printTable(ZeppelinSupport.scala:82)\n  ... 50 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 4 times, most recent failure: Lost task 0.3 in stage 57.0 (TID 455, cluster1-kafka-w-1.europe-west3-c.c.big-data-put.internal, executor 1): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604312_-821492118","id":"paragraph_1589352526148_-874877","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26374"},{"text":"degrees2.records.table.df.show(7)","user":"anonymous","dateUpdated":"2020-05-15T06:33:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:590)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:237)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n  at scala.collection.immutable.List.map(List.scala:298)\n  at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:590)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n  ... 50 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 4 times, most recent failure: Lost task 0.3 in stage 55.0 (TID 443, cluster1-kafka-w-1.europe-west3-c.c.big-data-put.internal, executor 1): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604313_2115626419","id":"paragraph_1589352539522_-734876568","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26375"},{"text":"%md\n## 4. Współczynnik klastrowania\nObliczany zgodnie z definicją, niestety znów nie można wyświetlić wyniku.","user":"anonymous","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4. Współczynnik klastrowania</h2>\n<p>Obliczany zgodnie z definicją, niestety znów nie można wyświetlić wyniku.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589352568527_-2026105043","id":"paragraph_1589352568527_-2026105043","dateCreated":"2020-05-15T07:28:14+0000","status":"FINISHED","$$hashKey":"object:26376","runtimeInfos":{}},{"text":"// Suma liczby istniejących krawędzi \n// między sąsiadami wierzchołka\n// dla wszystkich wierzchołków \nval rels = graph.cypher(\"MATCH (f:Person)-[:RELATED]->(n:Person), (f:Person)-[:RELATED]->(p:Person), (n)-[r:RELATED]->(p) RETURN count(r) AS cnt\")\nval relations: Long = rels.records.table.df.collect().map(_.getAs[Long](\"cnt\")).toSet.head\n\n// Count all persons\nval neigh = graph.cypher(\"MATCH (f:Person) RETURN count(f) AS cnt\")\nval neighbours: Long = neigh.records.table.df.collect().map(_.getAs[Long](\"cnt\")).toSet.head\n\n// Calculate \"clustering factor\"\nval factor = (relations.toFloat) / (neighbours * (neighbours - 1))","user":"anonymous","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589527365578_1590091335","id":"paragraph_1589527365578_1590091335","dateCreated":"2020-05-15T07:24:47+0000","status":"ERROR","focus":true,"$$hashKey":"object:32573","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.consume(SortMergeJoinExec.scala:36)\n  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:643)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:36)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:129)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:392)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2788)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)\n  ... 50 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 4 times, most recent failure: Lost task 0.3 in stage 16.0 (TID 253, cluster1-kafka-w-0.europe-west3-c.c.big-data-put.internal, executor 1): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://cluster1-kafka-m.europe-west3-c.c.big-data-put.internal:42009/jobs/job?id=13","$$hashKey":"object:32777"},{"jobUrl":"http://cluster1-kafka-m.europe-west3-c.c.big-data-put.internal:42009/jobs/job?id=14","$$hashKey":"object:32778"},{"jobUrl":"http://cluster1-kafka-m.europe-west3-c.c.big-data-put.internal:42009/jobs/job?id=15","$$hashKey":"object:32779"}],"interpreterSettingId":"spark"}}},{"text":"%md\n## 5. Średnia odległość do Kevina Bacona\nZawartość:\n1. Próba użycia funckji `shortestPath` - znalezienie najkrótszej ścieżki od Kevina Bacona do kazdego z wierzchołków, obliczanie średniej - nie działa (opisane w dokumentacji)\n2. Próba użycia bardziej topornego rozwiązania z funkcją `length` - dla każdego wierzchołka, obliczenie wszystkich sciezek do wszystkich wierzchołków, wybranie najkrótszej sciezki do kazdego wierzchołka, obliczenie średniej (tutaj potencjalnym problemem tego rozwiązania, poza nie zaimplementowaną funkcją length, jest bardzo duża złożoność).\n3. Próba stworzenia wyrażenia ścieżkowego - próba obejście nie zaimplementowanej funckji `length`, wyrażenie które przekazują ścieżkę do zmiennej również nie są zaimplementowane. Kwerenda pierwsza obrazuje jedyną działającą kwerendę którą udało się stworzyć.\nTeoretycznie możnaby szukac obejścia rozwiązania, np poprzez konstrukcję grafu pośredniego, ale podobnie jak w wypadku pkt. 2. będą wystepowac potencjalne problemy ze złożonością i z potrzebą użycia niezaimplementowanych funckji.","user":"anonymous","dateUpdated":"2020-05-15T06:45:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>5. Średnia odległość do Kevina Bacona</h2>\n<p>Zawartość:<br/>1. Próba użycia funckji <code>shortestPath</code> - znalezienie najkrótszej ścieżki od Kevina Bacona do kazdego z wierzchołków, obliczanie średniej - nie działa (opisane w dokumentacji)<br/>2. Próba użycia bardziej topornego rozwiązania z funkcją <code>length</code> - dla każdego wierzchołka, obliczenie wszystkich sciezek do wszystkich wierzchołków, wybranie najkrótszej sciezki do kazdego wierzchołka, obliczenie średniej (tutaj potencjalnym problemem tego rozwiązania, poza nie zaimplementowaną funkcją length, jest bardzo duża złożoność).<br/>3. Próba stworzenia wyrażenia ścieżkowego - próba obejście nie zaimplementowanej funckji <code>length</code>, wyrażenie które przekazują ścieżkę do zmiennej również nie są zaimplementowane. Kwerenda pierwsza obrazuje jedyną działającą kwerendę którą udało się stworzyć.<br/>Teoretycznie możnaby szukac obejścia rozwiązania, np poprzez konstrukcję grafu pośredniego, ale podobnie jak w wypadku pkt. 2. będą wystepowac potencjalne problemy ze złożonością i z potrzebą użycia niezaimplementowanych funckji.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604314_-1103680160","id":"paragraph_1589353351632_-2103251301","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:45:54+0000","dateFinished":"2020-05-15T06:45:54+0000","status":"FINISHED","$$hashKey":"object:26378"},{"text":"// jedyna działająca opcja - cały czas nie działa wyświetlanie wyników\nval kevin = graph.cypher(\n    \"\"\"|MATCH (kevin:Person {name: 'Kevin Bacon'})-[r:RELATED*0..2]-(target:Person {name: 'James Cameron'})\n      |RETURN target, r \"\"\".stripMargin\n  )","user":"anonymous","dateUpdated":"2020-05-15T06:47:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mkevin\u001b[0m: \u001b[1m\u001b[32morg.opencypher.okapi.relational.api.planning.RelationalCypherResult[org.opencypher.morpheus.impl.table.SparkTable.DataFrameTable]\u001b[0m = RelationalCypherResult(Some(Select(orderedFields=List(target :: NODE(:Person) @ session.tmp3, r :: LIST(RELATIONSHIP(:RELATED) @ session.tmp3)), solved=SolvedQueryModel(Set(target :: NODE(:Person) @ session.tmp3, kevin :: NODE(:Person) @ session.tmp3, r :: LIST(RELATIONSHIP(:RELATED) @ session.tmp3)),Set(target:Person :: BOOLEAN, kevin:Person :: BOOLEAN, target.name :: STRING? = $  AUTOSTRING1 :: STRING, kevin.name :: STRING? = $  AUTOSTRING0 :: STRING)))),Some(Select(expressions=List(target.name :: STRING?, target :: NODE(:Person) @ session.tmp3, target:Person :: BOOLEAN, r(1):RELATED :: BOOLEAN?, r(2):RELATED :: BOOLEAN?, ...\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604314_1449148848","id":"paragraph_1589354022742_-648911433","dateCreated":"2020-05-15T06:20:04+0000","dateStarted":"2020-05-15T06:47:08+0000","dateFinished":"2020-05-15T06:47:09+0000","status":"FINISHED","$$hashKey":"object:26379"},{"text":"kevin.records.table.df.show()","user":"anonymous","dateUpdated":"2020-05-15T06:47:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:189)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:187)\n  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:144)\n  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n  at org.apache.spark.sql.execution.BaseLimitExec.doProduce(limit.scala:70)\n  at org.apache.spark.sql.execution.BaseLimitExec.doProduce$(limit.scala:69)\n  at org.apache.spark.sql.execution.LocalLimitExec.doProduce(limit.scala:98)\n  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.LocalLimitExec.produce(limit.scala:98)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:590)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:237)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n  at scala.collection.immutable.List.map(List.scala:298)\n  at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:590)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.BaseLimitExec.inputRDDs(limit.scala:62)\n  at org.apache.spark.sql.execution.BaseLimitExec.inputRDDs$(limit.scala:61)\n  at org.apache.spark.sql.execution.LocalLimitExec.inputRDDs(limit.scala:98)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:590)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:237)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\n  at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:590)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)\n  ... 50 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 241, cluster1-kafka-w-1.europe-west3-c.c.big-data-put.internal, executor 2): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1892)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1880)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1879)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:274)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$2(BroadcastExchangeExec.scala:79)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:101)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:76)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:658)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  ... 3 more\nCaused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.predicates of type scala.collection.Seq in instance of org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n  at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2287)\n  at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1417)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2348)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n  at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2233)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589525176810_-716164915","id":"paragraph_1589525176810_-716164915","dateCreated":"2020-05-15T06:46:16+0000","dateStarted":"2020-05-15T06:47:13+0000","dateFinished":"2020-05-15T06:47:17+0000","status":"ERROR","$$hashKey":"object:26380"},{"text":"val k2 = graph.cypher(\n    \"\"\"|MATCH path=shortestPath((n:Person {name: 'Kevin Bacon'})-[*]-())\n       |RETURN path\"\"\".stripMargin\n    )","user":"anonymous","dateUpdated":"2020-05-15T06:35:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.opencypher.okapi.impl.exception.NotImplementedException: Support for pattern conversion of RelationshipChain(NodePattern(Some(Variable(n)),Vector(LabelName(Person)),None,None),RelationshipPattern(Some(Variable(  UNNAMED57)),List(),Some(None),None,BOTH,false,None),NodePattern(Some(Variable(  UNNAMED62)),Vector(),None,None)) not yet implemented\n  at org.opencypher.okapi.ir.impl.PatternConverter.$anonfun$convertElement$15(PatternConverter.scala:175)\n  at cats.data.StateFunctions.$anonfun$modify$4(IndexedStateT.scala:360)\n  at cats.data.StateFunctions.$anonfun$apply$2(IndexedStateT.scala:345)\n  at scala.Function1.$anonfun$andThen$1(Function1.scala:57)\n  at cats.data.IndexedStateT.$anonfun$run$1(IndexedStateT.scala:68)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.opencypher.okapi.ir.impl.PatternConverter.convert(PatternConverter.scala:58)\n  at org.opencypher.okapi.ir.impl.IRBuilderContext.convertPattern(IRBuilderContext.scala:61)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.$anonfun$convertPattern$1(IRBuilder.scala:546)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.EffInterpretation.$anonfun$run$1(Eff.scala:361)\n  at cats.Later.value$lzycompute(Eval.scala:150)\n  at cats.Later.value(Eval.scala:149)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.atnos.eff.EffInterpretation.run(Eff.scala:369)\n  at org.atnos.eff.EffInterpretation.run$(Eff.scala:351)\n  at org.atnos.eff.Eff$.run(Eff.scala:149)\n  at org.atnos.eff.syntax.EffNoEffectOps$.run$extension(eff.scala:59)\n  at org.opencypher.okapi.ir.impl.package$RichIRBuilderStack.run(package.scala:52)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.process(IRBuilder.scala:56)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.$anonfun$cypherOnGraph$5(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.time(RelationalCypherSession.scala:114)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.cypherOnGraph(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher(RelationalCypherGraph.scala:106)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher$(RelationalCypherGraph.scala:101)\n  at org.opencypher.okapi.relational.impl.graph.ScanGraph.cypher(ScanGraph.scala:43)\n  ... 50 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604314_2102747286","id":"paragraph_1589354202305_-1288270719","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26381"},{"text":"val k3 = graph.cypher(\n    \"\"\"|MATCH path=((n:Person {name: 'Kevin Bacon'})-[*]-(r))\n       |RETURN length(path)\"\"\".stripMargin\n    )","user":"anonymous","dateUpdated":"2020-05-15T06:35:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.opencypher.okapi.impl.exception.NotImplementedException: Support for pattern conversion of RelationshipChain(NodePattern(Some(Variable(n)),Vector(LabelName(Person)),None,None),RelationshipPattern(Some(Variable(  UNNAMED45)),List(),Some(None),None,BOTH,false,None),NodePattern(Some(Variable(r)),Vector(),None,None)) not yet implemented\n  at org.opencypher.okapi.ir.impl.PatternConverter.$anonfun$convertElement$15(PatternConverter.scala:175)\n  at cats.data.StateFunctions.$anonfun$modify$4(IndexedStateT.scala:360)\n  at cats.data.StateFunctions.$anonfun$apply$2(IndexedStateT.scala:345)\n  at scala.Function1.$anonfun$andThen$1(Function1.scala:57)\n  at cats.data.IndexedStateT.$anonfun$run$1(IndexedStateT.scala:68)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.opencypher.okapi.ir.impl.PatternConverter.convert(PatternConverter.scala:58)\n  at org.opencypher.okapi.ir.impl.IRBuilderContext.convertPattern(IRBuilderContext.scala:61)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.$anonfun$convertPattern$1(IRBuilder.scala:546)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.EffInterpretation.$anonfun$run$1(Eff.scala:361)\n  at cats.Later.value$lzycompute(Eval.scala:150)\n  at cats.Later.value(Eval.scala:149)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.atnos.eff.EffInterpretation.run(Eff.scala:369)\n  at org.atnos.eff.EffInterpretation.run$(Eff.scala:351)\n  at org.atnos.eff.Eff$.run(Eff.scala:149)\n  at org.atnos.eff.syntax.EffNoEffectOps$.run$extension(eff.scala:59)\n  at org.opencypher.okapi.ir.impl.package$RichIRBuilderStack.run(package.scala:52)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.process(IRBuilder.scala:56)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.$anonfun$cypherOnGraph$5(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.time(RelationalCypherSession.scala:114)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.cypherOnGraph(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher(RelationalCypherGraph.scala:106)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher$(RelationalCypherGraph.scala:101)\n  at org.opencypher.okapi.relational.impl.graph.ScanGraph.cypher(ScanGraph.scala:43)\n  ... 50 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604315_90804708","id":"paragraph_1589355270164_-1279109142","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26382"},{"text":"val k4 = graph.cypher(\n    \"\"\"|MATCH path=((n:Person {name: 'Kevin Bacon'})--(r))\n       |RETURN length(path)\"\"\".stripMargin\n    )","user":"anonymous","dateUpdated":"2020-05-15T06:35:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.opencypher.okapi.impl.exception.NotImplementedException: Support for converting function 'length' is not yet implemented\n  at org.opencypher.okapi.ir.impl.ExpressionConverter.convert(ExpressionConverter.scala:248)\n  at org.opencypher.okapi.ir.impl.IRBuilderContext.convertExpression(IRBuilderContext.scala:64)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.$anonfun$convertExpr$2(IRBuilder.scala:567)\n  at org.atnos.eff.Continuation.$anonfun$map$1(Continuation.scala:39)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.EffInterpretation.$anonfun$run$1(Eff.scala:361)\n  at cats.Later.value$lzycompute(Eval.scala:150)\n  at cats.Later.value(Eval.scala:149)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.atnos.eff.EffInterpretation.run(Eff.scala:369)\n  at org.atnos.eff.EffInterpretation.run$(Eff.scala:351)\n  at org.atnos.eff.Eff$.run(Eff.scala:149)\n  at org.atnos.eff.syntax.EffNoEffectOps$.run$extension(eff.scala:59)\n  at org.opencypher.okapi.ir.impl.package$RichIRBuilderStack.run(package.scala:52)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.process(IRBuilder.scala:56)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.$anonfun$cypherOnGraph$5(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.time(RelationalCypherSession.scala:114)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.cypherOnGraph(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher(RelationalCypherGraph.scala:106)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher$(RelationalCypherGraph.scala:101)\n  at org.opencypher.okapi.relational.impl.graph.ScanGraph.cypher(ScanGraph.scala:43)\n  ... 50 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604315_-1985215307","id":"paragraph_1589355312128_-2101266469","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26383"},{"text":"val k5 = graph.cypher(\n    \"\"\"|MATCH path=((n:Person {name: 'Kevin Bacon'})--(r))\n       |RETURN path\"\"\".stripMargin\n    )","user":"anonymous","dateUpdated":"2020-05-15T06:35:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.opencypher.okapi.impl.exception.NotImplementedException: Not yet able to convert expression: PathExpression(NodePathStep(Variable(n),SingleRelationshipPathStep(Variable(  UNNAMED45),BOTH,Some(Variable(r)),NilPathStep)))\n  at org.opencypher.okapi.ir.impl.ExpressionConverter.convert(ExpressionConverter.scala:313)\n  at org.opencypher.okapi.ir.impl.IRBuilderContext.convertExpression(IRBuilderContext.scala:64)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.$anonfun$convertExpr$2(IRBuilder.scala:567)\n  at org.atnos.eff.Continuation.$anonfun$map$1(Continuation.scala:39)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.EffInterpretation.$anonfun$run$1(Eff.scala:361)\n  at cats.Later.value$lzycompute(Eval.scala:150)\n  at cats.Later.value(Eval.scala:149)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.atnos.eff.EffInterpretation.run(Eff.scala:369)\n  at org.atnos.eff.EffInterpretation.run$(Eff.scala:351)\n  at org.atnos.eff.Eff$.run(Eff.scala:149)\n  at org.atnos.eff.syntax.EffNoEffectOps$.run$extension(eff.scala:59)\n  at org.opencypher.okapi.ir.impl.package$RichIRBuilderStack.run(package.scala:52)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.process(IRBuilder.scala:56)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.$anonfun$cypherOnGraph$5(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.time(RelationalCypherSession.scala:114)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.cypherOnGraph(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher(RelationalCypherGraph.scala:106)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher$(RelationalCypherGraph.scala:101)\n  at org.opencypher.okapi.relational.impl.graph.ScanGraph.cypher(ScanGraph.scala:43)\n  ... 50 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604315_-1802677025","id":"paragraph_1589355350974_-1261215317","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26384"},{"text":"val k6 = graph.cypher(\n    \"\"\"|MATCH p = ((kevin:Person {name: 'Kevin Bacon'})-[*0..2]-(target:Person {name: 'James Cameron'}))\n      |RETURN p \"\"\".stripMargin\n  )","user":"anonymous","dateUpdated":"2020-05-15T06:40:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.opencypher.okapi.impl.exception.NotImplementedException: Not yet able to convert expression: PathExpression(NodePathStep(Variable(kevin),MultiRelationshipPathStep(Variable(  UNNAMED48),BOTH,Some(Variable(target)),NilPathStep)))\n  at org.opencypher.okapi.ir.impl.ExpressionConverter.convert(ExpressionConverter.scala:313)\n  at org.opencypher.okapi.ir.impl.IRBuilderContext.convertExpression(IRBuilderContext.scala:64)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.$anonfun$convertExpr$2(IRBuilder.scala:567)\n  at org.atnos.eff.Continuation.$anonfun$map$1(Continuation.scala:39)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.EffInterpretation.$anonfun$run$1(Eff.scala:361)\n  at cats.Later.value$lzycompute(Eval.scala:150)\n  at cats.Later.value(Eval.scala:149)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.atnos.eff.EffInterpretation.run(Eff.scala:369)\n  at org.atnos.eff.EffInterpretation.run$(Eff.scala:351)\n  at org.atnos.eff.Eff$.run(Eff.scala:149)\n  at org.atnos.eff.syntax.EffNoEffectOps$.run$extension(eff.scala:59)\n  at org.opencypher.okapi.ir.impl.package$RichIRBuilderStack.run(package.scala:52)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.process(IRBuilder.scala:56)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.$anonfun$cypherOnGraph$5(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.time(RelationalCypherSession.scala:114)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.cypherOnGraph(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher(RelationalCypherGraph.scala:106)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher$(RelationalCypherGraph.scala:101)\n  at org.opencypher.okapi.relational.impl.graph.ScanGraph.cypher(ScanGraph.scala:43)\n  ... 50 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604315_921155037","id":"paragraph_1589355390470_-38055126","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26385"},{"text":"val k7 = graph.cypher(\n    \"\"\"|MATCH p = ((kevin:Person {name: 'Kevin Bacon'})-[r]-(target:Person {name: 'James Cameron'}))\n      |RETURN p \"\"\".stripMargin\n  )","user":"anonymous","dateUpdated":"2020-05-15T06:40:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.opencypher.okapi.impl.exception.NotImplementedException: Not yet able to convert expression: PathExpression(NodePathStep(Variable(kevin),SingleRelationshipPathStep(Variable(r),BOTH,Some(Variable(target)),NilPathStep)))\n  at org.opencypher.okapi.ir.impl.ExpressionConverter.convert(ExpressionConverter.scala:313)\n  at org.opencypher.okapi.ir.impl.IRBuilderContext.convertExpression(IRBuilderContext.scala:64)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.$anonfun$convertExpr$2(IRBuilder.scala:567)\n  at org.atnos.eff.Continuation.$anonfun$map$1(Continuation.scala:39)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.Interpret.$anonfun$runInterpreter$1(Interpret.scala:43)\n  at org.atnos.eff.Continuation.go$1(Continuation.scala:54)\n  at org.atnos.eff.Continuation.apply(Continuation.scala:72)\n  at org.atnos.eff.EffInterpretation.$anonfun$run$1(Eff.scala:361)\n  at cats.Later.value$lzycompute(Eval.scala:150)\n  at cats.Later.value(Eval.scala:149)\n  at cats.Eval$.loop$1(Eval.scala:347)\n  at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368)\n  at cats.Eval$FlatMap.value(Eval.scala:307)\n  at org.atnos.eff.EffInterpretation.run(Eff.scala:369)\n  at org.atnos.eff.EffInterpretation.run$(Eff.scala:351)\n  at org.atnos.eff.Eff$.run(Eff.scala:149)\n  at org.atnos.eff.syntax.EffNoEffectOps$.run$extension(eff.scala:59)\n  at org.opencypher.okapi.ir.impl.package$RichIRBuilderStack.run(package.scala:52)\n  at org.opencypher.okapi.ir.impl.IRBuilder$.process(IRBuilder.scala:56)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.$anonfun$cypherOnGraph$5(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.time(RelationalCypherSession.scala:114)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherSession.cypherOnGraph(RelationalCypherSession.scala:164)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher(RelationalCypherGraph.scala:106)\n  at org.opencypher.okapi.relational.api.graph.RelationalCypherGraph.cypher$(RelationalCypherGraph.scala:101)\n  at org.opencypher.okapi.relational.impl.graph.ScanGraph.cypher(ScanGraph.scala:43)\n  ... 50 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589523604316_1975851848","id":"paragraph_1589355452495_602591284","dateCreated":"2020-05-15T06:20:04+0000","status":"READY","$$hashKey":"object:26386"},{"text":"%md\n# Wnioski\nIstnieją dwa główne problemy związane z zaimplementowaniem tych rozwiązań w Morpheusu i Zeppelinie. \nUdało nam się rozwiązać zadanie nr 3. (choć nie udało się wyświetlić wyniku za pomoca Zeppelina). Jednocześnie udało się rozwiązać zadania 1 i 2 (mimo niemożności wyswietlenia wyniku), jednak należy zwróić uwagę, że rozwiązania te funkcjonują, dzięki faktowi istnienia wierzchołków bez żadnych krawędzi wychodzących/wchodzących, które można relatywnie łatwo zidentyfikować. W innych sytuacjach wykonanie tego zadania zależałoby od implementacji algorytmów grafowych w Morpheusu (obecnie nie zaimplementowane).\n\nRozwiązanie zadania numer 4 działa w spark-shellu, ale w Zeppelinie jak widac niekoniecznie.\n\nJeśli chodzi o zadanie numer 5, w związku z brakiem wsparcia dla wyrażeń ścieżkowych nie da się wykonać tego zadania w sposób standardowy. Byćmoże istnieją pośrednie sposoby wyznaczenia takiej odległości, ale w związku z brakiem bezpośredniego wsparcia, takie rozwiązanie może być bardzo nieefektywne.","user":"anonymous","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Wnioski</h1>\n<p>Istnieją dwa główne problemy związane z zaimplementowaniem tych rozwiązań w Morpheusu i Zeppelinie.<br/>Udało nam się rozwiązać zadanie nr 3. (choć nie udało się wyświetlić wyniku za pomoca Zeppelina). Jednocześnie udało się rozwiązać zadania 1 i 2 (mimo niemożności wyswietlenia wyniku), jednak należy zwróić uwagę, że rozwiązania te funkcjonują, dzięki faktowi istnienia wierzchołków bez żadnych krawędzi wychodzących/wchodzących, które można relatywnie łatwo zidentyfikować. W innych sytuacjach wykonanie tego zadania zależałoby od implementacji algorytmów grafowych w Morpheusu (obecnie nie zaimplementowane).</p>\n<p>Rozwiązanie zadania numer 4 działa w spark-shellu, ale w Zeppelinie jak widac niekoniecznie.</p>\n<p>Jeśli chodzi o zadanie numer 5, w związku z brakiem wsparcia dla wyrażeń ścieżkowych nie da się wykonać tego zadania w sposób standardowy. Byćmoże istnieją pośrednie sposoby wyznaczenia takiej odległości, ale w związku z brakiem bezpośredniego wsparcia, takie rozwiązanie może być bardzo nieefektywne.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589353369268_-773187718","id":"paragraph_1589353369268_-773187718","dateCreated":"2020-05-15T07:36:09+0000","status":"FINISHED","$$hashKey":"object:26387","runtimeInfos":{}},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-05-15T07:34:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1589528041143_-1869872548","id":"paragraph_1589528041143_-1869872548","dateCreated":"2020-05-15T07:34:01+0000","status":"READY","focus":true,"$$hashKey":"object:32824"}],"name":"morpheus-sga","id":"2F8YDV2QT","defaultInterpreterGroup":"spark","version":"0.9.0-SNAPSHOT","permissions":{},"noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{},"path":"/morpheus-sga"}